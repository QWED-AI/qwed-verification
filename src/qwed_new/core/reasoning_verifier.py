"""
Engine 8: Reasoning Verifier.
Validates that LLMs correctly understand natural language queries before execution.
"""

import re
import json
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass

from qwed_new.providers.azure_openai import AzureOpenAIProvider
from qwed_new.providers.anthropic import AnthropicProvider

@dataclass
class ReasoningValidation:
    """Result of reasoning verification."""
    is_valid: bool
    confidence: float  # 0.0 to 1.0
    reasoning_trace: List[str]
    issues: List[str]
    primary_formula: str
    alternative_formula: Optional[str] = None
    semantic_facts: Optional[Dict[str, Any]] = None

class ReasoningVerifier:
    """
    Engine 8: Validates LLM understanding of queries.
    
    Uses multi-LLM cross-validation to catch "translation errors"
    where the LLM generates the wrong formula for a correctly-stated problem.
    """
    
    def __init__(self):
        self.primary_llm = AzureOpenAIProvider()  # GPT-4
        self.secondary_llm = AnthropicProvider()  # Claude
        
    def verify_understanding(
        self,
        query: str,
        primary_task: Any,  # MathVerificationTask or similar
        enable_cross_validation: bool = True
    ) -> ReasoningValidation:
        """
        Validate that the LLM correctly understood the query.
        
        Args:
            query: Original natural language query
            primary_task: The task generated by the primary LLM
            enable_cross_validation: Whether to use secondary LLM for comparison
            
        Returns:
            ReasoningValidation with confidence and issues
        """
        issues = []
        
        # 1. Extract semantic facts from query
        facts = self._extract_semantic_facts(query)
        
        # 2. Generate reasoning trace from primary LLM
        reasoning_trace = self._generate_reasoning_trace(query, primary_task)
        
        # 3. Validate formula semantics
        formula_issues = self._validate_formula_semantics(facts, primary_task.expression)
        issues.extend(formula_issues)
        
        # 4. Cross-validate with secondary LLM (if enabled)
        alternative_formula = None
        if enable_cross_validation:
            try:
                secondary_task = self.secondary_llm.translate(query)
                alternative_formula = secondary_task.expression
                
                # Compare formulas
                if not self._formulas_equivalent(primary_task.expression, secondary_task.expression):
                    issues.append(f"LLM disagreement: GPT='{primary_task.expression}' vs Claude='{secondary_task.expression}'")
            except Exception as e:
                issues.append(f"Cross-validation failed: {str(e)}")
        
        # Calculate confidence
        confidence = self._calculate_confidence(issues, facts, reasoning_trace)
        
        return ReasoningValidation(
            is_valid=len(issues) == 0,
            confidence=confidence,
            reasoning_trace=reasoning_trace,
            issues=issues,
            primary_formula=primary_task.expression,
            alternative_formula=alternative_formula,
            semantic_facts=facts
        )
    
    def _extract_semantic_facts(self, query: str) -> Dict[str, Any]:
        """Extract entities, numbers, and operations from query."""
        facts = {
            "numbers": [],
            "entities": [],
            "operations": [],
            "keywords": []
        }
        
        # Extract numbers
        numbers = re.findall(r'\b\d+\.?\d*\b', query)
        facts["numbers"] = [float(n) for n in numbers]
        
        # Extract operation keywords
        operation_keywords = {
            "add": ["add", "plus", "sum", "total", "together"],
            "subtract": ["subtract", "minus", "less", "remove", "eat", "lose"],
            "multiply": ["multiply", "times", "of", "each"],
            "divide": ["divide", "per", "split", "share"]
        }
        
        for op_type, keywords in operation_keywords.items():
            for keyword in keywords:
                if keyword in query.lower():
                    facts["operations"].append(op_type)
                    facts["keywords"].append(keyword)
        
        # Extract named entities (simplified - just proper nouns)
        words = query.split()
        for word in words:
            if word and word[0].isupper() and len(word) > 1:
                facts["entities"].append(word)
        
        return facts
    
    def _generate_reasoning_trace(self, query: str, task: Any) -> List[str]:
        """Force LLM to explain its reasoning step-by-step."""
        prompt = f"""Given this problem:
"{query}"

You generated the formula: {task.expression}

Explain your reasoning step-by-step. For each step, state:
1. What information you extracted
2. What operation you performed
3. Why you chose that operation

Format as a numbered list."""
        
        try:
            # Use GPT-4 for reasoning trace
            response = self.primary_llm.client.chat.completions.create(
                model=self.primary_llm.deployment,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500
            )
            
            trace_text = response.choices[0].message.content
            # Parse into list
            lines = trace_text.split('\n')
            trace = [line.strip() for line in lines if line.strip() and (line[0].isdigit() or line.startswith('-'))]
            
            return trace if trace else ["No reasoning trace generated"]
        except Exception as e:
            return [f"Failed to generate reasoning trace: {str(e)}"]
    
    def _validate_formula_semantics(self, facts: Dict, formula: str) -> List[str]:
        """Check if the formula makes semantic sense given the facts."""
        issues = []
        
        # Check: Are all numbers from the query in the formula?
        formula_numbers = re.findall(r'\b\d+\.?\d*\b', formula)
        formula_numbers_set = set(float(n) for n in formula_numbers)
        query_numbers_set = set(facts["numbers"])
        
        missing_numbers = query_numbers_set - formula_numbers_set
        if missing_numbers:
            issues.append(f"Formula missing numbers from query: {missing_numbers}")
        
        # Check: Do operations match keywords?
        # e.g., if query says "buy 3 times", formula should have multiplication
        if "times" in facts["keywords"] or "multiply" in facts["operations"]:
            if "*" not in formula and "**" not in formula:
                issues.append("Query mentions multiplication but formula doesn't contain '*'")
        
        if "subtract" in facts["operations"] or "eat" in facts["keywords"]:
            if "-" not in formula:
                issues.append("Query mentions subtraction but formula doesn't contain '-'")
        
        return issues
    
    def _formulas_equivalent(self, formula1: str, formula2: str) -> bool:
        """
        Check if two formulas are semantically equivalent.
        Returns True if they're the same or trivially equivalent.
        """
        # Simple normalization
        f1 = formula1.replace(" ", "").lower()
        f2 = formula2.replace(" ", "").lower()
        
        if f1 == f2:
            return True
        
        # Check commutative equivalence (a+b == b+a)
        # Simplified check for now
        return False
    
    def _calculate_confidence(
        self,
        issues: List[str],
        facts: Dict,
        reasoning_trace: List[str]
    ) -> float:
        """Calculate confidence score based on validation results."""
        if not issues:
            return 1.0  # Perfect
        
        # STRICTER: Penalty for each issue type (increased from original)
        confidence = 1.0
        
        for issue in issues:
            if "missing numbers" in issue.lower():
                confidence -= 0.4  # Increased from 0.3
            elif "disagreement" in issue.lower():
                confidence -= 0.5  # Increased from 0.4 (LLM disagreement is VERY serious)
            elif "operation" in issue.lower():
                confidence -= 0.3  # Increased from 0.2
            elif "formula" in issue.lower():
                confidence -= 0.3  # New penalty for formula issues
            else:
                confidence -= 0.15  # Increased from 0.1
        
        # Additional penalty if reasoning trace is weak
        if len(reasoning_trace) < 2:
            confidence -= 0.2
        
        return max(confidence, 0.0)
